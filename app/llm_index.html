<!DOCTYPE html>
<html>

<link rel="stylesheet" href="styles\llm_home.css">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" rel="stylesheet">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<head>
    <title>Commercially Available LLM Models</title>
</head>

<body>
    <!-- <h1 class="page-title">General LLM Model information for Commercially Available Models</h1> -->
    <h1 class="table-title">Commercially Available LLM Models</h1>
    <h3 class="general-overview">The following table provides you with general information on the LLM's that are currently commercially 
        available on the market, their context window size, input and output token costs, and where applicable the 
        knowledge base date that the model's information is based on.</h3>
    <hr/>
    <h2 class="table-cols-explained">LLM Models Table Explained</h2>
    
        <h3 class="context-title">Context Window</h3>
        <p class="context-explained">The context or context window is the amount of information that's taken as an input for a query by
            the user. The length of the context window is based on token length. Each token is equivalent to four
            characters or 3/4 of a word in English, with 100 words being equivalent to 75 words.
            
            Consider the following; smaller input lengths will result in the LLM having less input to process, 
            which results in faster input generation.
        </p>

        <h3 class="input-output-title">Input (Prompt Pricing) & Output (Completion Pricing)</h3>
        <p class="input-explained">The LLM models seen below have varying prices and are calculated based on the number of tokens that are
            used to process the request. The prices are usually based per million tokens (1MTok). The input tokens are the number of tokens used
            to process the input that the user is giving the LLM model via an input prompt. 

        <!-- <br>
            Meanwhile, the output tokens are calculated based on the number of tokens needed to generate the response that 
            was requested by the user. The cost for using an LLM is therefore calculated based on the total number of input and output tokens used.
        </p> -->
        <p class="output-explained">Meanwhile, the output tokens are calculated based on the number of tokens needed to generate the response that 
            was requested by the user. The cost for using an LLM is therefore calculated based on the total number of input and output tokens used.
        </p>

        <h3 class="knowledge-date-title">Knowledge Base Date</h3>
        <p class="knowledge-date-explained">The knowledge base date outlines the date at which the training data for the model was gathered and trained on.
            After the date, the model wouldn't be aware of any information or events that took place unless it's updated or retrained on the newer data.
        </p>
    <table class="content-table">
        <thead>
            <tr>
                <th>Company</th>
                <th>Model Name</th>
                <th>Context</th>
                <th>Input ($/1M Tokens)</th>
                <th>Output ($/1M Tokens)</th>
                <th>Knowledge Base Date</th>
            </tr>
        </thead>
        <tbody>
            <!-- Injected LLM model dataset here -->
        </tbody>
    </table>

    <p class="last-updated">Last updated: June 5th, 2024</p>
</body>

</html>